<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这里主要介绍一些常见的预训练模型背后的模型，比如Bert，RoBERTa, Transformer-XL, XLNet, ELECTRA 。至于现在能写到哪里就算是到哪里吧">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp_attention">
<meta property="og:url" content="http://yoursite.com/2020/07/07/nlp-attention/index.html">
<meta property="og:site_name" content="Mongoose&#39;s blog">
<meta property="og:description" content="这里主要介绍一些常见的预训练模型背后的模型，比如Bert，RoBERTa, Transformer-XL, XLNet, ELECTRA 。至于现在能写到哪里就算是到哪里吧">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/transformer%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/attention%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/RoBERTa%E4%B8%AD%E5%85%B3%E4%BA%8ENSP%E7%9A%84%E5%AE%9E%E9%AA%8C.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/transformer-xl%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8segment-length-4.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/%E6%99%AE%E9%80%9Aattention%E4%B8%ADQK%E7%9A%84%E4%B9%98%E7%A7%AF%E5%B1%95%E5%BC%80.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/transformer-xl%E4%B8%ADQK%E7%9A%84%E4%B9%98%E7%A7%AF%E5%B1%95%E5%BC%80.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/transformer-xl%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/BERT%E6%A8%A1%E5%9E%8B%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="http://yoursite.com/2020/07/07/nlp-attention/xlnet-%E4%B8%A4%E9%98%B6%E6%AE%B5attention.png">
<meta property="article:published_time" content="2020-07-07T14:51:44.000Z">
<meta property="article:modified_time" content="2020-07-07T14:51:44.000Z">
<meta property="article:author" content="Mongooses">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="attention">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="bert">
<meta property="article:tag" content="roberta">
<meta property="article:tag" content="transformer-xl">
<meta property="article:tag" content="xlnet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/07/nlp-attention/transformer%E7%BB%93%E6%9E%84.png">

<link rel="canonical" href="http://yoursite.com/2020/07/07/nlp-attention/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>nlp_attention | Mongoose's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mongoose's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/07/nlp-attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mongooses">
      <meta itemprop="description" content="recording something">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mongoose's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          nlp_attention
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-07 22:51:44" itemprop="dateCreated datePublished" datetime="2020-07-07T22:51:44+08:00">2020-07-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这里主要介绍一些常见的预训练模型背后的模型，比如Bert，RoBERTa, Transformer-XL, XLNet, <del>ELECTRA</del> 。至于现在能写到哪里就算是到哪里吧</p>
<span id="more"></span>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>参考论文:</p>
<ul>
<li><strong>Attention Is All You Need</strong> : 2017年发表</li>
<li><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>： 2018/2019</li>
</ul>
<p>RNN/LSTM这之类的模型中任意两个输入或者输出之间的操作是和他们的距离成正比的，这就使得模型很难学习到这些长距离的关系。这正是attention(准确是多头注意力)出现的主要原因，在此处中的transformer其实只依赖于self-attention</p>
<img src="/2020/07/07/nlp-attention/transformer%E7%BB%93%E6%9E%84.png" class="" title="transformer模型结构">
<ul>
<li>encoder：包含$N = 6$个这样的结构。这个结构分为两个sub-layer结构，第一个sub-layer就是multi-head self-attention。第二个sub-layer就是简单的position-wise fully connected feed-forward network。其实每个sub-layer结构都会有一个residual connection(残差连接)，然后在进行normalization。也就是$LayerNorm(x + Sublayer(x))$.</li>
<li>decoder：也包含这样6个这样的结构</li>
</ul>
<p>所谓的attention其实就是如下的结构</p>
<img src="/2020/07/07/nlp-attention/attention%E7%BB%93%E6%9E%84.png" class="" title="attention结构">
<p>单个的attention(具体来说scaled Dot-Product Attention)其实就是：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中的softmax中的除以的分母就是scale. 做这个的目的还是因为$QK^T$这个操作会使得结果矩阵中的每个数字变得比较大，除以$\sqrt{d_k}$可以一定程度上缓解这个问题。</p>
<p>至于多头是指</p>
<script type="math/tex; mode=display">
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O</script><p>where</p>
<script type="math/tex; mode=display">
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>在这篇文章中，transformer将多头的注意力机制的使用方式分为三种</p>
<ul>
<li>在<code>encoder-decoder attention</code>层中，query来自于上一个decoder layer中的内容，key和value是来自于encoder的输出。</li>
<li>encoder包含这self-attention层。这些部分的所有的key和value以及query都是来自于相同的地方，也就是encoder上一层中的输出。</li>
<li>当然在decoder中，self-attention来时允许来自当前位置及其左侧的内容，所以需要进行一系列的屏蔽mask操作，其实就是mask成$-\infin$. 这样的话，才能在softmax之后，将这个位置的给屏蔽掉。</li>
</ul>
<p>使用代码实现应该如下(这里的<code>einsum</code>用的真是出神入化，此外，这部分代码来自于transformer-xl的部分，这里还有些迷惑性的操作，比如<code>forward</code>中的<code>c</code>和<code>h</code>以及<code>mems</code>，个人感觉只是为了transformer-xl本身的需要，而不是原始的transformer的需要；不清楚抱抱脸那个是怎么实现的)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttn</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_head, dropout, dropatt=<span class="number">0</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                 pre_lnorm=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttn, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_head = d_head</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.q_net = nn.Linear(d_model, n_head * d_head, bias=<span class="literal">False</span>)</span><br><span class="line">        self.kv_net = nn.Linear(d_model, <span class="number">2</span> * n_head * d_head, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.dropatt = nn.Dropout(dropatt)</span><br><span class="line">        self.o_net = nn.Linear(n_head * d_head, d_model, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">        self.scale = <span class="number">1</span> / (d_head ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.pre_lnorm = pre_lnorm</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h, attn_mask=<span class="literal">None</span>, mems=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment">##### multihead attention</span></span><br><span class="line">        <span class="comment"># [hlen x bsz x n_head x d_head]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mems <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            c = torch.cat([mems, h], <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c = h</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_lnorm:</span><br><span class="line">            <span class="comment">##### layer normalization</span></span><br><span class="line">            c = self.layer_norm(c)</span><br><span class="line"></span><br><span class="line">        head_q = self.q_net(h)</span><br><span class="line">        head_k, head_v = torch.chunk(self.kv_net(c), <span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        head_q = head_q.view(h.size(<span class="number">0</span>), h.size(<span class="number">1</span>), self.n_head, self.d_head)</span><br><span class="line">        head_k = head_k.view(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), self.n_head, self.d_head)</span><br><span class="line">        head_v = head_v.view(c.size(<span class="number">0</span>), c.size(<span class="number">1</span>), self.n_head, self.d_head)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [qlen x klen x bsz x n_head]</span></span><br><span class="line">        attn_score = torch.einsum(<span class="string">&#x27;ibnd,jbnd-&gt;ijbn&#x27;</span>, (head_q, head_k))</span><br><span class="line">        attn_score.mul_(self.scale)</span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> attn_mask.<span class="built_in">any</span>().item():</span><br><span class="line">            <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">                attn_score.masked_fill_(attn_mask[<span class="literal">None</span>,:,:,<span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">            <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">                attn_score.masked_fill_(attn_mask[:,:,:,<span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [qlen x klen x bsz x n_head]</span></span><br><span class="line">        attn_prob = F.softmax(attn_score, dim=<span class="number">1</span>)</span><br><span class="line">        attn_prob = self.dropatt(attn_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -&gt; [qlen x bsz x n_head x d_head]</span></span><br><span class="line">        attn_vec = torch.einsum(<span class="string">&#x27;ijbn,jbnd-&gt;ibnd&#x27;</span>, (attn_prob, head_v))</span><br><span class="line">        attn_vec = attn_vec.contiguous().view(</span><br><span class="line">            attn_vec.size(<span class="number">0</span>), attn_vec.size(<span class="number">1</span>), self.n_head * self.d_head)</span><br><span class="line"></span><br><span class="line">        <span class="comment">##### linear projection</span></span><br><span class="line">        attn_out = self.o_net(attn_vec)</span><br><span class="line">        attn_out = self.drop(attn_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_lnorm:</span><br><span class="line">            <span class="comment">##### residual connection</span></span><br><span class="line">            output = h + attn_out</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">##### residual connection + layer normalization</span></span><br><span class="line">            output = self.layer_norm(h + attn_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>前面解释了多头感知机，另外一个部分就是后面所谓的<code>Position-wise Feed-Forward Networks</code>，写作公式应该是</p>
<script type="math/tex; mode=display">
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2</script><p>使用代码描述就是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff=<span class="number">2048</span>, dropout = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># We set d_ff as a default to 2048</span></span><br><span class="line">        self.linear_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.dropout(F.relu(self.linear_1(x)))</span><br><span class="line">        x = self.linear_2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>Embedding and Softmax： 没太看懂啥意思。好奇的话，看原始论文吧</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax √ linear transformation,</p>
<p>不知道说的啥</p>
</blockquote>
<p>另外, 和之前一样, 在embedding层那部分也除了$\sqrt{d_{model}}$</p>
<p>Position Embedding，这个主要是为了解决attention结构中无法体现位置信息的缺点而加上的。简单来说其实就是</p>
<script type="math/tex; mode=display">
PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">
PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, max_seq_len = <span class="number">200</span>, dropout = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># create constant &#x27;pe&#x27; matrix with values dependant on </span></span><br><span class="line">        <span class="comment"># pos and i</span></span><br><span class="line">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class="line">        <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, d_model, <span class="number">2</span>):</span><br><span class="line">                pe[pos, i] = \</span><br><span class="line">                math.sin(pos / (<span class="number">10000</span> ** ((<span class="number">2</span> * i)/d_model)))</span><br><span class="line">                pe[pos, i + <span class="number">1</span>] = \</span><br><span class="line">                math.cos(pos / (<span class="number">10000</span> ** ((<span class="number">2</span> * (i + <span class="number">1</span>))/d_model)))</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># 做这个是因为来的是一个batch, 要让前面让出一维进行广播</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># make embeddings relatively larger</span></span><br><span class="line">        x = x * math.sqrt(self.d_model)</span><br><span class="line">        <span class="comment">#add constant to embedding</span></span><br><span class="line">        seq_len = x.size(<span class="number">1</span>)</span><br><span class="line">        pe = Variable(self.pe[:,:seq_len], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> x.is_cuda:</span><br><span class="line">            pe.cuda()</span><br><span class="line">        x = x + pe</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>为什么采用self-attention</p>
<p>主要是和rnn和卷积来比较的, 从 </p>
<ol>
<li>每一层的复杂度</li>
<li>计算能否并行化</li>
<li>网络中两个长依赖的路径的长度</li>
</ol>
<p>略,实际上我也没太看明白</p>
<p>训练流程（参考性不太强，因为它的训练时task-specify的，这可能是为什么后来又了BERT的原因）：</p>
<ol>
<li>优化器Adam, $\beta_1=0.9, \beta_2 = 0.98, \epsilon=10^{-9}$</li>
<li>$lrate = d_{model}^{-0.5}\cdot{min(step_num^{-0.5}, step_num\cdot{warmup_steps^{-1.5}})}$, 其中$warmup_steps = 4000$, 训练步骤大概为300000个step</li>
<li>在每次加到sub-layer的输入之前或者normalized之前都会进行dropout，同时在加embedding的时候，包括encoder和decoder的时候也进行了dropout，$P_{drop}=0.1$</li>
</ol>
<hr>
<p>接下来继续介绍BERT相关的东西了，上面还都是介绍Transformer</p>
<blockquote>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
<p>简单来说，使用了Transformer结构的预训练模型</p>
</blockquote>
<p>预训练语言模型的常见种类(随便看看)</p>
<ol>
<li>无监督feature-based方法。典型例子：ELMo，简单介绍，和BERT十分像，但是BERT使用Transformer来作为基准的模型结构，但是ELMo使用的双向LSTM。个人觉着：这里是想强调，使用ELMo来生成特征向量，然后就进行下层的具体任务了，不会再训练ELMo这个模型本身了</li>
<li>无监督Fine-tuneing方法。典型例子，OpenAI的GPT，这个模型十分神奇，它使用了Transformer这种结构，准确来说只用到了之前提到的Transformer的decoder部分。它与训练的时候，使用前面的文字来预测下一个文字，也就是说对于i位置的文字的生成(没错，就是<code>Improving Language Understandingby Generative Pre-Training</code>中的<code>Generative</code>)不会参考i+1及其之后的内容。他事实上并没有什么神奇的，但是它确实是BERT之前就出现的与训练语言模型，BERT反而是后面来的。关于ELMo到GPT再到BERT的演进，可以看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54578457">此处</a>。个人感觉：这里是想强调，这里使用与训练好的GPT模型，添加或者不添加额外的模型结构或者冻结或者不冻结现有结构等等方法，然后使用它本身训练的方法来在此在你本身的数据集上做一下微调。</li>
<li>从监督数据中来进行迁移学习。个人感觉：就是使用模型，在添加额外的下游任务层(或者不添加)，根据下游任务的标签来做迁移学习某些层或者全部层的东西。</li>
</ol>
<p>BERT：1. pre-training(与训练). 2. fine-tuning(微调). 具体来说，先在无标签的数据上进行与训练。然后再fine-tuning的阶段，BERT模型先初始化成这些预训练好的参数，这些参数再使用有标签的数据进行具体的下游任务进行训练。(废话)</p>
<p>BERT预训练是通过两个不同的任务来做的</p>
<ol>
<li><strong>Masked LM</strong>, 其实就是随机mask掉一些token，然后预测这些token。在BERT原始实验中，mask掉15%的wordpiece过的token，然后最终预测求loss的时候，只算那些被mask掉的部分的预测结果。因为在fine-tune阶段的时候，看不到[MASK]这个token，所以说，这里的做法是如果一个词需要被mask掉了，那么这个词有80%的实践使用<code>[MASK]</code>提到，10%的时候，使用随机的token来替代，10%的时间不改变这个词。</li>
<li><strong>Next Sentence Prediction (NSP)</strong>。这个就简单了点，就是使用<code>[CLS]</code>处的词向量来计算B是否是A的下一个句子，在处理数据的时候，B是有50%的概率是A的下一句。虽然这篇文章中说这个对于下游的QA和NLI任务有好处，但是后续的几个模型，认为这个NSP似乎用处不大，这个就后面再说了。</li>
</ol>
<p>那么如何进行fine-tune呢？略，真的fine-tune的大概都知道怎么做</p>
<p>这里简单说说如何使用BERT来做阅读理解问答任务。</p>
<p>在原始的BERT论文中，他们使用A作为问题文本，使用B作为篇章文本来进行拼接。使用两个向量权重$S\in{R^H}$和$E$，分别用来计算开头和结尾的概率$P_i = \frac{e^{S\cdot{T_i}}}{\sum_je^{S\cdot{T_j}}}$，如果是结尾的话，使用$E$，那么使用$S\cdot{T_i} + E\cdot{T_i}$的最大值的$[i:j + 1]$来做答案的片段。</p>
<p>对于Squad2.0的数据集，在上述的数据集中还包含了无法回答的问题。这个时候就需要判断这个问题是否有答案，那么可以使用$S\cdot{C} + E\cdot{C}$这个数值是否大于之前答案片段的数值那个最大值，如果是的话，那么就没有答案。甚至也可以加个$\tau$的delta。</p>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p>
</blockquote>
<p>主要在原始的BERT的预训练中，修改了如下的几个点</p>
<ol>
<li>训练模型更长时间，使用更大的batch(256)，使用更多的数据</li>
<li>去除NSP训练方式（next sentence prediction objective）</li>
<li>训练更长的序列(512 tokens)</li>
<li>在训练集中动态的更改mask的模式: 说来很简单，将一个句子进行10种mask，然后将每种mask后的句子再复制4次，所以一条原始数据最终变成了40个，其中只有10种不同的变换，所以会有遇到相同的模式四次。</li>
</ol>
<p>关于NSP的实验结果</p>
<img src="/2020/07/07/nlp-attention/RoBERTa%E4%B8%AD%E5%85%B3%E4%BA%8ENSP%E7%9A%84%E5%AE%9E%E9%AA%8C.png" class="" title="RoBERTa关于NSP的实验结果">
<p>这里有几个需要关注的点（事实上，文中也说了）</p>
<ol>
<li>使用独立的句子会破坏下游的任务</li>
<li>去除NSP loss会稍微提高一下下游任务（我感觉这点有点奇怪，为什么不直接拿SEGMENT-PAIR不带NSP loss来预训练，然后做下游的fine-tune测试呢？这样不应该更加直观嘛）</li>
</ol>
<p>Text Encoding： 采用的BPE</p>
<p>那么这里定义什么是RoBERTa(的预训练)</p>
<ol>
<li><p>动态mask</p>
</li>
<li><p>full-sentences without NSP</p>
</li>
<li><p>更大的mini-batches，更搭的byte-level BPE(这个不细说了，对于中文来说不是那么重要了)</p>
<p>关于batch size，这里有个数量级可供参考，原始的bert预训练的batch size为256， 但是fine-tune的时候大概是32.</p>
<p>但是再RoBERTa中，预训练的batch size级别是8k级别的。</p>
</li>
</ol>
<p>总之，他的训练结果十分不错，详细的可以参考论文。这里不再多说了，这些训练的指标本身不是很重要，但是谁好谁坏，在某个数据集或者某个领域内可能有些重要。</p>
<h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h2><blockquote>
<p><strong>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</strong></p>
<p>2019</p>
</blockquote>
<p>Transformer有什么什么缺陷？有一些论文会提出很多的想法，但是在日常使用中，有一个缺点是十分直观的：文档的长度不能过长，过长的话，显存直接就会爆掉，事实上，很多现在效果比较好的预训练BERT类的模型都需要梯度累计好几次才能达到32的batch size，如果文档过长的话，可能batch size 为1都放不下。</p>
<p>这就是Transformer-XL最直观的贡献就在这个地方。</p>
<ol>
<li>解决了不做长期的dependency</li>
<li>也解决了上下文分裂的问题</li>
</ol>
<p>事实上，这篇论文中，其实只有两个比较大的点</p>
<ol>
<li>如何表示segment-level，并且还得重复用到之前的state。 对应于3.2</li>
<li>既然表示成了segment了，那么position embedding怎么办.对应于3.3</li>
</ol>
<p>对于如何表示成segment-level，表示如下</p>
<p>划定一个窗口编号$\tau$，里面包含着$L$长度的上下文，表示为$s<em>\tau = [x</em>{\tau, 1}, …, x<em>{\tau, L}]$，则下一个窗口就应该表述成$s</em>\tau = [x<em>{\tau+1, 1}, …, x</em>{\tau+1, L}]$。我们将第$\tau$个段落的第$n$层的hidden state表述成$h_\tau^n\in{R^{L\times{d}}}$，那么第$\tau+1$这个段落在第$n$层产生的hidden state 就应该使用下面的公式来表述(仅仅用于示意，不是真实这么用的，个人感觉主要是错开的距离和他论文中的图不一样)</p>
<script type="math/tex; mode=display">
\widetilde{h_{\tau+1}^{n-1}} = [SG(h_\tau^{m-1})\circ{h_{\tau+1}^{n-1}}]</script><script type="math/tex; mode=display">
q_{\tau+1}^n, k_{\tau+1}^n, v_{\tau+1}^n = h_{\tau+1}^{n-1}W_q^T, \widetilde{h_{\tau+1}^{n-1}}W_k^T, \widetilde{h_{\tau+1}^{n-1}}W_v^T</script><script type="math/tex; mode=display">
h_{\tau+1}^n = Transformer-Layer(q_{\tau+1}^n, k_{\tau+1}^n, v_{\tau+1}^n)</script><p>其中的$SG(\bullet)$表示的是这部分的数据就不做梯度反传了，其中$[h<em>u\circ{h_v}]$表示是按照$L$这个维度连接起来，也就是对于这个而例子来说$\widetilde{h</em>{\tau+1}^{n-1}}\in{R^{2L\times{d}}}$, 值得注意的是对于$q\in{R^{L\times{d}}}$， $k\in{R^{2L\times{d}}}$， $v\in{R^{2L\times{d}}}$, 则最终计算的结果$h_{\tau+1}^n\in{R^{L\times{d}}}$</p>
<p>这里敲公式敲得我手疼，但是我觉着有必要解释一下，上面的公式是示意的，总感觉有点怪怪的，和下面的示例有些不同，下面的每个位置，包括同一个segment下的不同的位置，相同的层数使用的上一个segment的向量都是不同的，因为在同一个segment中，不同的位置，也就对应了上一个segment中不同的位置，那么就得一列一列走，这和segment有什么关系？这里不是特别理解</p>
<img src="/2020/07/07/nlp-attention/transformer-xl%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8segment-length-4.png" class="" title="transformer-xl模型使用了segment-length-为4的图示">
<p>第二个就是相对位置的embedding的解决问题。</p>
<img src="/2020/07/07/nlp-attention/%E6%99%AE%E9%80%9Aattention%E4%B8%ADQK%E7%9A%84%E4%B9%98%E7%A7%AF%E5%B1%95%E5%BC%80.png" class="" title="普通attention中QK的乘积的展开">
<img src="/2020/07/07/nlp-attention/transformer-xl%E4%B8%ADQK%E7%9A%84%E4%B9%98%E7%A7%AF%E5%B1%95%E5%BC%80.png" class="" title="transformer-xl中QK的乘积展开">
<p>主要替换的就是蓝色和红色的部分</p>
<ol>
<li>蓝色部分，使用了相对位置编码$R$来进行编码，这里的编码就是原始的Transformer这个论文中的余弦函数的编码方式，不需要学习</li>
<li>红色部分，由于原始的那个乘积实际上是个固定值，或者说对于每个窗口内都是相同的值，或者说，如果不进行梯度反传更新参数的话，这边的值是不变的，或者说是他们如果发生变化，实际上只应该体现在一个变量中就行了，不必写成两个变量，这两个红色的值，是可以训练的。<strong>这部分是无论与query的位置都应该是固定的。</strong></li>
<li>将Wk的值分来来计算。</li>
</ol>
<p>这里给出一些代码来辅助解释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">klen = w_head_k.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           <span class="comment"># qlen x bsz x n_head x d_head</span></span><br><span class="line">w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           <span class="comment"># qlen x bsz x n_head x d_head # 个人感觉这里有问题，应该是k_len x ... 下同，根据论文来说，k_len大概是2倍的q_len</span></span><br><span class="line">w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           <span class="comment"># qlen x bsz x n_head x d_head</span></span><br><span class="line"></span><br><span class="line">r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                <span class="comment"># qlen x n_head x d_head</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### compute attention score r_w_bias 为u</span></span><br><span class="line">rw_head_q = w_head_q + r_w_bias                                         <span class="comment"># qlen x bsz x n_head x d_head # 这里实际上将AC项分开看了，后一半就是普通的K， i按一般需要加个bias</span></span><br><span class="line">AC = torch.einsum(<span class="string">&#x27;ibnd,jbnd-&gt;ijbn&#x27;</span>, (rw_head_q, w_head_k))             <span class="comment"># qlen x klen x bsz x n_head</span></span><br><span class="line"></span><br><span class="line">rr_head_q = w_head_q + r_r_bias <span class="comment"># r_r_bias 是 v</span></span><br><span class="line">BD = torch.einsum(<span class="string">&#x27;ibnd,jnd-&gt;ijbn&#x27;</span>, (rr_head_q, r_head_k))              <span class="comment"># qlen x klen x bsz x n_head</span></span><br><span class="line">BD = self._rel_shift(BD)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [qlen x klen x bsz x n_head]</span></span><br><span class="line">attn_score = AC + BD</span><br><span class="line">attn_score.mul_(self.scale)</span><br></pre></td></tr></table></figure>
<p>这里面还有个<code>_rel_shift</code>的操作，这个操作可以参考原始论文的附录B</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_rel_shift</span>(<span class="params">self, x, zero_triu=<span class="literal">False</span></span>):</span></span><br><span class="line">    zero_pad = torch.zeros((x.size(<span class="number">0</span>), <span class="number">1</span>, *x.size()[<span class="number">2</span>:]),</span><br><span class="line">                           device=x.device, dtype=x.dtype)</span><br><span class="line">    x_padded = torch.cat([zero_pad, x], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x_padded = x_padded.view(x.size(<span class="number">1</span>) + <span class="number">1</span>, x.size(<span class="number">0</span>), *x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">    x = x_padded[<span class="number">1</span>:].view_as(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> zero_triu:</span><br><span class="line">        ones = torch.ones((x.size(<span class="number">0</span>), x.size(<span class="number">1</span>)))</span><br><span class="line">        x = x * torch.tril(ones, x.size(<span class="number">1</span>) - x.size(<span class="number">0</span>))[:,:,<span class="literal">None</span>,<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这里给出一个感性的认识，我们在求A和C部分的时候，事实上我们都不在乎相对位置，但是对于BD的时候，对于query，我们是全部都看的（长度是L），但是对于key的地方，这个时候其实长度是L+M的，但是我们不看当前位置后面的key内容，所以需要将其变成0，或者说这个时候连query都不看了，因为key后面补上0的话，即使前面有query，也都变成了0。</p>
<p>至于为什么这么写，我虽然看懂了他这么操作的目的，但是对于这么操作就能够完成这个功能，还是有些迷惑。或许你能在<a href="www.linzehui.me/2019/05/07/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E5%85%B3%E4%BA%8Etransformer-xl%E4%B8%ADrel-shift%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E8%AF%BB/">此</a>获得一些灵感。</p>
<p>但是根据这个公式也能给出一些灵感来，就是到底这个操作是一个列列来的，还是一个segment一个segment来的。</p>
<blockquote>
<p>都不太正确，如果按照我的说法来说，应该是：</p>
<p>不考虑相对位置的地方是按照一个segment一个segment来的，比如AC</p>
<p>但是对于考虑query和key的相对位置的时候，比如BD。从逻辑上是一列一列来，但是可以写成一个segment的窗口写法</p>
</blockquote>
<p>所以最终transformer-xl的最终公式应该写成</p>
<img src="/2020/07/07/nlp-attention/transformer-xl%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png" class="" title="transformer-xl计算公式">
<p>这个公式有点意思就是$SG(m_{\tau}^{n-1})$中并没有使用$h$。<del>如果是按照这个公式来看的话，实际上，就不是一列一列来的，还是一个segment一个segment来的。</del> </p>
<h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><blockquote>
<p><strong>XLNet: Generalized Autoregressive Pretraining for Language Understanding</strong></p>
</blockquote>
<p>首先比较自回归模型和bert模型的区别</p>
<img src="/2020/07/07/nlp-attention/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%AC%E5%BC%8F.png" class="" title="自回归模型的公式">
<img src="/2020/07/07/nlp-attention/BERT%E6%A8%A1%E5%9E%8B%E5%85%AC%E5%BC%8F.png" class="" title="BERT模型公式">
<p>自回归模型是1，BERT模型是2，他们分别的优势和缺陷是</p>
<ol>
<li>独立性假设。对于BERT模型(2)，有独立性假设。但是对于1也就是自回归模型没有。这里还需要考虑一件事情，对于被mask掉的token1的预测其实也不依赖于被mask掉的token2的预测结果，这样是不是有些问题呢？</li>
<li>输入噪声。在BERT模型中，因为要使用<code>[MASK]</code>来隐藏掉某些token，这样才能进行预测，但是在下游的任务中，这些<code>[MASK]</code>字符肯定是不存在的。但是自回归模型就没有这个问题。</li>
<li>上下文独立。自回归模型不如BERT模型能够获取整个文本的上下文，自回归模型只有上文</li>
</ol>
<img src="/2020/07/07/nlp-attention/xlnet-%E4%B8%A4%E9%98%B6%E6%AE%B5attention.png" class="" title="xlnet-两阶段attention">
<p>在我看来，xlnet就是在transformer-xl的基础上，进行了两个attention，一个包含当前元素，一个不包含当前元素。具体是通过mask来实现的。(上面的图片来自论文作者的<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1zJ411P7X6">视频讲解</a>)</p>
<h2 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h2><p>略</p>
<p>越来越懒了，大概使用了GAN的方法，一个简单的生成模型来将一个具有mask的文本进行补全，就行bert预训练那样，然后使用辨别器去辨别生成出来的句子是否正确，在绝大多数下游任务中，只需要辨别器去预测。</p>
<p>如果有兴趣可以去看看原始论文，或者看看知乎上的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89763176">一篇文章</a>,或者<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112813856">这个</a>， 尤其是后者，有很多人评论这个模型的一些话还是有些道理的。</p>
<h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p>越来越懒了，只写我粗略看了这篇文章的观后感</p>
<ol>
<li>在embedding那层中就用了两个矩阵的乘法代替之前比较大的embedding矩阵</li>
<li>使用sop，而不是NSP来做预训练（当然，普通的mask预测缺失的词的做法还在），sop是指预测句子顺序是否是前后关系</li>
<li>句子的长度也更长了</li>
<li>好像mask的时候也不是随机mask了，也采用了mask掉一段文本(n-gram)的操作（忘记了，抱歉）</li>
<li>共享了所有的层，其实准确来说，试验了各种的共享方式，比如只共享ffn，只共享attention，或者都共享，看看效果怎么样</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>attention/transformer</code>解决了基础模型结构</p>
<p><code>bert</code>第一个(准确来说是第二个？GPT好像是第一个)使用<code>attention/transformer</code>预训练的比较好的语言模型</p>
<p><code>RoBERTa</code>相当于又训练一遍<code>bert</code>，只是稍微改进一下训练方式</p>
<p><code>transformer-xl</code>某种程度上解决了长文本的问题</p>
<p><code>xlnet</code>虽然基于<code>transformer-xl</code>，但是实际上她最主要的贡献还是在于两个阶段的attention，相当于将pretrained和finetune阶段都放在了一块</p>
<p><code>ALBERT</code>是代表着压缩BERT的一个思路，核心是共享参数</p>
<p><code>ELECTRA</code>使用了GAN来做与训练语言模型，于普通的GAN不同的是，这里需要辨别器更加强大一些</p>
<p>话说，你知道我为啥只写了这几个模型吗？因为这几个是有预训练好的模型的，所以可以直接用，别的看得再多也没啥用，真的从头预训练一遍吗？</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/attention/" rel="tag"># attention</a>
              <a href="/tags/transformer/" rel="tag"># transformer</a>
              <a href="/tags/bert/" rel="tag"># bert</a>
              <a href="/tags/roberta/" rel="tag"># roberta</a>
              <a href="/tags/transformer-xl/" rel="tag"># transformer-xl</a>
              <a href="/tags/xlnet/" rel="tag"># xlnet</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/07/29/conda%20+%20pipenv/" rel="prev" title="conda + pipenv 配置深度学习开发环境">
      <i class="fa fa-chevron-left"></i> conda + pipenv 配置深度学习开发环境
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/08/lstm/" rel="next" title="lstm知识点">
      lstm知识点 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT"><span class="nav-number">1.</span> <span class="nav-text">BERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RoBERTa"><span class="nav-number">2.</span> <span class="nav-text">RoBERTa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-XL"><span class="nav-number">3.</span> <span class="nav-text">Transformer-XL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XLNet"><span class="nav-number">4.</span> <span class="nav-text">XLNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELECTRA"><span class="nav-number">5.</span> <span class="nav-text">ELECTRA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ALBERT"><span class="nav-number">6.</span> <span class="nav-text">ALBERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mongooses</p>
  <div class="site-description" itemprop="description">recording something</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mongooses</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
