---
title: 在机器学习和深度学习中如何处理"大数据"
tags: 
    - pytables
    - bcols
    - blaze
    - 大数据
    - 深度学习
---

在机器学习或者深度学习中，很多时候数据集会很大，这时候不能够一次性的载入内存，虽然有迭代器这样的东西加上分块存储数据一定程度上解决了这个问题，甚至，有时候这样简单的做法也十分推荐，毕竟不是每个开发者都有精力去学习一门新的工具去特殊处理数据集。

绝大多数的深度学习框架都能很好的api来封装自己的dataset，但是实际上，我们还有一些别的工具来来特殊存储这些比较大的数据。

1. 借助大数据中的hdf5格式，可以使用pytables，显而易见的好处是能和大数据平台比较好的交互，事实上，别的大数据平台下的数据格式(parquet，但是需要相应的python包来操作)也可以使用

2. 特殊的on-disk或者on-memory格式，bcolz，我个人认为最大的优势是不用像1中的方法那样需要定义列的数据类型(值得说明的是1中也不是必须定义数据类型，至少在python环境下)

3. 如果你感觉如此之多的格式我无法一一学习，那么可以使用blaze来具体**处理操作**这些数据，但是这样是否又是额外新加了一层？是否又得学一个东西？

2与其他的包的不同是它定义了一种内存/disk格式，可以很方便的将数据载入内存或者硬盘。更在于一种`存储格式`，而不是一种对数据的操作，它的api绝大多数都是在寻找、迭代数据，而不是在操作数据(虽然有eval这个函数来做吧)

1更多的还是在读取/写入`hdf5`格式

3更多的是统一一个数据操作格式的接口，更多的在`操作`和`统一接口`

<!--more-->

## bcolz more details

> bcolz 基于numpy、分块压缩存储在内存或者硬盘中，支持导入导出HDF5/PyTables tables/pandas DataFrame

事实上，我只用过bcolz，如果仅从读取或者存储数据的角度而言，它十分便捷，几乎和numpy的array一样方便，而且几乎不需要考虑内存问题，我个人对于pandas比较熟悉，所以bcolz的一些缺点我可以很方便的使用pandas来解决

我十分喜欢用这玩意从原始的数据中分批读入数据，创造我需要的feature 以及 label，删除读入内存中的原始数据、创建bcolz中的carray格式，只要及时删除读入内存中的原始数据，我就不用担心内存问题，因为bcolz的格式会自动处理这些问题，在处理完所有的数据之后，直接flush到disk中。之后我只需要一次性读入disk中的那个bcolz存入的文件夹，然后迭代出来就可以做训练了，我根本不需要担心内存问题。

当然，如果你的列是不同的类型，可能hdf5这样能够定义列名称的比较适合(或者bcolz中的ctable)，我所说的使用bcolz(carray)的场景更像是图片一类的数据

## 未完待续